{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнє завдання на тему Spark SQL\n",
    "\n",
    "Задачі з домашнього завдання на SQL потрібно розвʼязати за допомогою Spark SQL DataFrame API.\n",
    "\n",
    "Дампи таблиць знаходяться в папці `data`.\n",
    "Можете створювати стільки нових клітинок, скільки вам необхідно.\n",
    "\n",
    "Розвʼязок кожної задачі має бути відображений в самому файлі (використати метод `.show()`)\n",
    "\n",
    "**Увага!** Використовувати мову запитів SQL безпосередньо забороняється, потрібно використовувати виключно DataFrame API!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Вивести кількість фільмів в кожній категорії.\n",
    "Результат відсортувати за спаданням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film_category = spark.read.csv('home_task/data/film_category.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film = spark.read.csv('home_task/data/film.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "category = spark.read.csv('home_task/data/category.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "joined_df = film_category.join(film, film_category.film_id == film.film_id, \"inner\") \\\n",
    "    .join(category, film_category.category_id == category.category_id) \\\n",
    "    .select(category.name, film.film_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       name|sum_film_id|\n",
      "+-----------+-----------+\n",
      "|     Sports|      39584|\n",
      "|    Foreign|      35720|\n",
      "|Documentary|      35461|\n",
      "|      Drama|      32562|\n",
      "|  Animation|      32427|\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_sort_df = joined_df.groupBy(\"name\").agg(f.sum(\"film_id\").alias(\"sum_film_id\")).orderBy(f.col(\"sum_film_id\").desc())\n",
    "group_sort_df.show(5)                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Вивести 10 акторів, чиї фільми брали на прокат найбільше.\n",
    "Результат відсортувати за спаданням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film_actor = spark.read.csv('home_task/data/film_actor.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film = spark.read.csv('home_task/data/film.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actor = spark.read.csv('home_task/data/actor.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "joined_df = actor.join(film_actor, film_actor.actor_id == actor.actor_id) \\\n",
    "    .join(film, film_actor.film_id == film.film_id) \\\n",
    "    .select(actor.first_name, actor.last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+\n",
      "|first_name|  last_name|count|\n",
      "+----------+-----------+-----+\n",
      "|     SUSAN|      DAVIS|   54|\n",
      "|      GINA|  DEGENERES|   42|\n",
      "|    WALTER|       TORN|   41|\n",
      "|      MARY|     KEITEL|   40|\n",
      "|   MATTHEW|     CARREY|   39|\n",
      "|    SANDRA|     KILMER|   37|\n",
      "|  SCARLETT|      DAMON|   36|\n",
      "|    ANGELA|WITHERSPOON|   35|\n",
      "|     HENRY|      BERRY|   35|\n",
      "|    VIVIEN|   BASINGER|   35|\n",
      "+----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_sort_df = joined_df.groupBy(\"first_name\", \"last_name\").count().orderBy(f.col(\"count\").desc()).limit(10)\n",
    "group_sort_df.show(20)                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "Вивести категорія фільмів, на яку було витрачено найбільше грошей\n",
    "в прокаті"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inventory = spark.read.csv('home_task/data/inventory.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film_category = spark.read.csv('home_task/data/film_category.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "joined_df = payment.join(rental, payment.customer_id == rental.customer_id) \\\n",
    "    .join(inventory, rental.inventory_id == inventory.inventory_id) \\\n",
    "    .join(film_category, inventory.film_id == film_category.film_id) \\\n",
    "    .join(category, film_category.category_id == category.category_id) \\\n",
    "    .select(category.name, payment.amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|  name|        sum_amount|\n",
      "+------+------------------+\n",
      "|Sports|138295.47000005541|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_sort_df = joined_df.groupBy(\"name\").agg(f.sum(\"amount\").alias(\"sum_amount\")).orderBy(f.col(\"sum_amount\").desc()).limit(1)\n",
    "group_sort_df.show(5)                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "Вивести назви фільмів, яких не має в inventory.\n",
    "Запит має бути без оператора IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inventory = spark.read.csv('home_task/data/inventory.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film = spark.read.csv('home_task/data/film.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "joined_df = film.join(inventory, film.film_id == inventory.film_id, \"left_outer\").filter(f.col(\"inventory_id\").isNull()).select(film.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|      ALICE FANTASIA|\n",
      "|         APOLLO TEEN|\n",
      "|      ARGONAUTS TOWN|\n",
      "|       ARK RIDGEMONT|\n",
      "|ARSENIC INDEPENDENCE|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_sort_df = joined_df.orderBy(f.col(\"title\"))\n",
    "group_sort_df.show(5)                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "Вивести топ 3 актори, які найбільше зʼявлялись в категорії фільмів “Children”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actor = spark.read.csv('home_task/data/actor.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film_actor = spark.read.csv('home_task/data/film_actor.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film_category = spark.read.csv('home_task/data/film_category.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "category = spark.read.csv('home_task/data/category.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "joined_df = actor.join(film_actor, actor.actor_id == film_actor.actor_id, \"left\") \\\n",
    "    .join(film_category, film_category.film_id == film_actor.film_id, \"left\") \\\n",
    "    .join(category, category.category_id == film_category.category_id, \"left\") \\\n",
    "    .filter(category.name == \"Children\") \\\n",
    "    .select( actor.first_name, actor.last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+----------+\n",
      "|first_name|last_name|count|row_number|\n",
      "+----------+---------+-----+----------+\n",
      "|     HELEN|   VOIGHT|    7|         1|\n",
      "|     SUSAN|    DAVIS|    6|         2|\n",
      "|     KEVIN|  GARLAND|    5|         3|\n",
      "+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.orderBy(f.col(\"count\").desc())\n",
    "group_sort_df = joined_df.groupBy(\"first_name\", \"last_name\").count() \\\n",
    "    .withColumn(\"row_number\",f.row_number().over(windowSpec)) \\\n",
    "    .orderBy(f.col(\"count\").desc()) \\\n",
    "    .filter(f.col(\"row_number\") < 4)\n",
    "group_sort_df.show(10)                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "Вивести міста з кількістю активних та неактивних клієнтів\n",
    "(в активних customer.active = 1).\n",
    "Результат відсортувати за кількістю неактивних клієнтів за спаданням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "customer = spark.read.csv('home_task/data/customer.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "address = spark.read.csv('home_task/data/address.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "city = spark.read.csv('home_task/data/city.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+\n",
      "|               city|unactive|active|\n",
      "+-------------------+--------+------+\n",
      "|          Xiangfan_|       1|     0|\n",
      "|           Wroclaw_|       1|     0|\n",
      "|          Uluberia_|       1|     0|\n",
      "|       Szkesfehrvr_|       1|     0|\n",
      "|   Southend-on-Sea_|       1|     0|\n",
      "|         Pingxiang_|       1|     0|\n",
      "|         Najafabad_|       1|     0|\n",
      "|        Kumbakonam_|       1|     0|\n",
      "|            Ktahya_|       1|     0|\n",
      "|            Kamyin_|       1|     0|\n",
      "|            Daxian_|       1|     0|\n",
      "|     Coatzacoalcos_|       1|     0|\n",
      "|  Charlotte Amalie_|       1|     0|\n",
      "|           Bat Yam_|       1|     0|\n",
      "|            Amroha_|       1|     0|\n",
      "|_A Corua (La Corua)|       0|     1|\n",
      "|              _Abha|       0|     1|\n",
      "|         _Abu Dhabi|       0|     1|\n",
      "|             _Adana|       0|     1|\n",
      "|              _Acua|       0|     1|\n",
      "+-------------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unactive_df = customer.join(address, customer.address_id == address.address_id, \"left\") \\\n",
    "    .join(city, address.city_id == city.city_id, \"left\") \\\n",
    "    .filter(customer.active == 0) \\\n",
    "    .groupBy(city.city).count() \\\n",
    "    .withColumnRenamed(\"count\", \"unactive\") \\\n",
    "    .withColumnRenamed(\"city\", \"city_unactive\")\n",
    "\n",
    "active_df = customer.join(address, customer.address_id == address.address_id, \"left\") \\\n",
    "    .join(city, address.city_id == city.city_id, \"left\") \\\n",
    "    .filter(customer.active == 1) \\\n",
    "    .groupBy(city.city).count() \\\n",
    "    .withColumnRenamed(\"count\", \"active\") \\\n",
    "    .withColumnRenamed(\"city\", \"city_active\")\n",
    "\n",
    "full_df = unactive_df.join(active_df, unactive_df.city_unactive == active_df.city_active, \"fullouter\")\n",
    "full_df_1 = full_df.fillna(\"\").na.fill(value=0, subset=[\"unactive\", \"active\"])\n",
    "full_df_2 = full_df_1.select(f.concat(\"city_unactive\", f.lit(\"_\"), \"city_active\").alias(\"city\"), \"unactive\", \"active\")\n",
    "full_df_3 = full_df_2.orderBy(f.col(\"city_unactive\").desc())\n",
    "full_df_3.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
